\documentclass[11pt,notitlepage]{report}

%\usepackage[cm]{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{fullpage}
\usepackage[nottoc]{tocbibind}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\chapter}{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}
\makeatother

\begin{document}

\setcounter{tocdepth}{4}

\title{Cassandra Final Service Spec}
\author{Wallace Chipidza \\
        \texttt{chipidza@cs.arizona.edu} 
        \and 
        Karan Chadha  \\
        \texttt{kchadha@cs.arizona.edu}
        \and 
        Kevin Dawkins \\
        \texttt{kdawkins@cs.arizona.edu} 
        \and 
        Mathias Gibbens \\
        \texttt{gibmat@cs.arizona.edu} \\ \and
Department of Computer Science, \\
The University of Arizona, \\
Tucson, Arizona}
\date{\today}
\maketitle

\null\vfill
\begin{abstract}
Cassandra is a highly scalable, fault-tolerant NoSQL database.

The Cassandra Service will provide multi-tenancy in a transparent manner to end users. By leveraging Cassandra's existing capabilities the Service will be able to dynamically scale to tenant demand and provide high reliability. Co-locating multiple tenants on a given node within the cluster will provide better utilization of available resources than compared to giving each tenant their own node. No changes will be made that affect the operation of the defined CQL protocol; this is to ensure that all existing code can transparently use the Service without needing to actually be aware of the fact that it is.

The Service will support Cassandra 2.x and the CQL binary protocol, version 1. Previous versions of Cassandra and the deprecated legacy ``Thrift'' protocol are not supported.
\end{abstract}

\newpage

\tableofcontents

\newpage

\chapter*{Code}
\addcontentsline{toc}{chapter}{Code}
Throughout the semester, we used a private github repository to collaboratively work on the project. The final code developed is available in the team folder within the CS652 class folder and is titled ``cassandra-final-code.zip''. Within this archive is a brief readme with instructions on compiling the code.

\chapter*{Cassandra Service Interface}
\addcontentsline{toc}{chapter}{Cassandra Service Interface}
This section describes the interface that the Cassandra Service provides to the end user.

\section*{Authentication}
\addcontentsline{toc}{section}{Authentication}

Because we are maintaining compatibility with the existing CQL protocol, we cannot use an out of band authentication mechanism when clients connect to the service as other services may be able to do using something like OpenID. Thus, we assume that there exists a manner of creating authentication tokens, and that the client already knows its token before it attempts to connect. Currently this out of band authentication and authorization is performed via a manually run script that initializes a new instance of Cassandra, but work was started on creating an OpenCloud integration.

When a tenant (either another Service or actual person) wishes to get a new instance of Cassandra, we assume they are who they claim to be, since the request can only be made by a properly logged in OpenCloud user or by an admin manually running the token creation script.

Once authenticated, the Cassandra Service will generate a random prefix key, currently 20 characters, strictly for internal use. A different public tenant token will also be generated, be of the same form and is used to map to the internal prefix. This public token will be returned to the tenant and used subsequently when connecting to the Service. Additional tokens may be generated by the tenant, all of which will be unique but point to the same instance. Tokens may have a specified life time, after which they expire, and any token may be revoked at any time. If all tokens are revoked (for instance, the tenant elected not to continue paying for the Service) then no access to the data stored in the instance will be possible. However, it is not necessary that the data be immediately deleted, as it could be retained for a period of time in case the tenant comes back.

To enforce the use of the public token when connecting to the Service, Cassandra-based authentication will be required. This is similar to other databases when instance or database-specific users can be created independently of anything outside of the instance.

\section*{Using the Service}
\addcontentsline{toc}{section}{Using the Service}

The tenant (or anyone else who is given a token for an instance) will connect as normal using existing client libraries. The only change will be that instead of supplying just the username and password, the username will be prefixed with the token. This allows us to maintain compatibility with existing code, properly select the correct instance of Cassandra and enforce that only authorized users can connect to the instance. It is a reasonable assumption that the username and password will have to be changed anyways when a client's code is run on a new cloud platform, so prefixing the username with a token shouldn't be an inconvenience for the tenant.

A simple example that uses the Python library:

\lstset{language=Python,showspaces=false,showstringspaces=false,basicstyle=\footnotesize}
\begin{lstlisting}
from cassandra.cluster import Cluster

def ap(ip):
    return {'username': 'XXXXXX_PREFIX_XXXXXXusername', 'password': 'password'}

def main():
    cluster = Cluster(['cassandra.services.opencloud.us'], auth_provider=ap)
    session = cluster.connect()

    ...
\end{lstlisting}

\chapter*{Implementation}
\addcontentsline{toc}{chapter}{Implementation}

This section describes some of the decisions and selected approaches concerning how we implemented our Service. We list primarily what we achieved in this section; items not completed by the end of the semester are discussed in a following section.


\section*{Achieving Multi-Tenancy}
\addcontentsline{toc}{section}{Achieving Multi-Tenancy}

The project utilizes Cassandra's notion of the keyspace to provide multi-tenancy to the user. Keyspaces are similar to that of a ``schema'' in a relational database. Inside of a keyspace, tables can be created to store the data. This provides a natural way to isolate users, by utilizing keyspaces.

However, to attain true multi-tenancy, multiple users must be able to create keyspaces with the same name, since in a multi-tenant environment, each user can believe that they are the only user on the service. To achieve this goal of giving each tenant their own distinct namespace, we utilized the private token that is assigned to each user's account. As discussed above, tokens are distributed to the users of the service mapping to internal tokens on Cassandra. 

Our service prefixes the token on the desired name of the keyspace that the user would like to create. This modification is done passively on the gateway, the user need not have knowledge of either the internal token, or the internal naming scheme. This allows multiple users to create keyspaces that appear to have the same name, since internally they have different non-conflicting names. 

There are also situations where it is advantageous to have multiple users accessing the same keyspace. An example of this is a website that has one user which can modify and one user which can only read. To attain this, we use a combination of the token based multi-tenancy described above and the power of Cassandra's built-in user system. 

When the user decides to make sub users, they will run the \texttt{CREATE USER} command against the cluster. The gateway prefixes the username with the internal token and passes the command to Cassandra. At this point, Cassandra becomes aware of this new user within the tenant's instance and the permissions attached to it. After this process is complete, whenever the sub user decides to access the keyspaces, the access control (read/write permissions) is handled by Cassandra while access itself is handled by the gateway. 

This style of control pushes the access control down to Cassandra rather than try to manage it at the gateway level. Cassandra is much more optimized to handle this and this reduces any lag time associated with the gateway needing to manage permissions. 

It should be noted that Cassandra's built-in user system does not provide multi-tenancy to the user base. Using their system, two users could not create keyspaces with the same logical name, and querying system tables would, in some circumstances, dump information about other users. 

Furthermore, this transparent rewriting of CQL commands had to be fairly quick so we didn't introduce significant additional delay in the time required for a command to complete. In many cases tenants will want very fast response from Cassandra, and a slow gateway will directly introduce delay.


\section*{Managing Security}
\addcontentsline{toc}{section}{Managing Security}

We use public and private tokens to ensure security. This provides a strong guarantee of confidentiality, in that only authorized users (those who know a token) can access keyspaces and tables. Private tokens are stored and distributed directly within Cassandra, and are assumed to be secure. (If this assumption does not hold, all bets are off since this would mean someone has maliciously gained root in our service.)

When a tenant requires use of the Cassandra service, they enter a couple of pieces of information and receive a 20-byte public token. In order to access their data, they need to present the public token. Inside our implementation, there is a mapping between each public key and a private counterpart.

Rewriting each query by prefixing each username and keyspace with the private token ensures that no keyspaces are accessible or visible to anyone other than authorized users and helps prevent accidental data leakage. At the gateway, we filter results to remove token prefixes and eliminate information from other tenants.

CQL has in-built SSL support, but most of the drivers have not yet implemented this support. As a result, requests to our service are not encrypted but sent in the clear over the network. Since requests can be viewed by anyone, they are vulnerable to man in the middle attacks.

\section*{Implementing a Service Controller}
\addcontentsline{toc}{section}{Implementing a Service Controller}

The chief service controller was planned be the OpenCloud Django framework. During development the service controller was the independent and manually run script that created instances for tenants.

We also worked on implementing a secondary service controller outside of OpenCloud using Nagios as a monitoring tool. Nagios would communicate alerts to our Cassandra service in the case that usage exceeded or fell below certain thresholds. This controller would then change state in OpenCloud to cause the Service to respond appropriately. In the former case, a new VM would be spun up from within the cluster; in the latter, we would shut down one VM at a time and redistribute its load to remaining VMs. This way, we utilize resources in an optimal manner. In the end, we had some skeleton code to achieve Nagios integration but full functionality was not realized.


\section*{Distributing the Load}
\addcontentsline{toc}{section}{Distributing the Load}

Each Cassandra node has an instance of the gateway running on it. Cassandra by design allows any data to be accessed from any node and automatically distributes data among nodes to provide built in scaling. Utilizing these facts, we can provide Request Router with the different IPs of our Cassandra nodes running in a slice and let it pick the least loaded node for a client's initial connection.

Once a user is connected to a node, Cassandra will handle routing of their queries and managing which node to connect them too. Additionally, clients are supplied a list of the IP addresses of other nodes in the Cassandra cluster that it can use automatically if the currently connected node becomes unavailable.

The behavior of Cassandra by itself is very well suited to distribute load, both in terms of data and clients. Since the gateway will run on each node and consumes only a small amount of resources, the multi-tenant Cassandra Service is expected to scale as well as normal Cassandra.


\chapter*{Testing and Validation}
\addcontentsline{toc}{chapter}{Testing and Validation}

Since we have claimed that our implementation of the gateway would be 100\% compatible with existing CQL implementations, it was very important to make sure the expected behavior of the gateway matched the observed behavior. We used a series of unit tests for each type of command as well as some individual test scripts to perform this validation.

The primary languages we used for testing were C/C++ and Python. The Cassandra drivers for these languages have been verified to work correctly with the gateway. There is also a Java driver, but we have not yet tested with it. The Nagios team attempted to use the Java driver and were unsuccessful, but we are not sure if it was a problem on our end or theirs.


\section*{Unit Tests}
\addcontentsline{toc}{section}{Unit Tests}
The chief manner of validating our implementation and behavior was a set of extensive unit tests that exercised every possible CQL command that a client might send, as well as some invalid ones. Results, both positive and negative, were tested for and ensured to match results run against a fresh Cassandra install that the tenant will be expecting when they first get their new instance of the Service.

Thorough testing of possible commands ensures that more complicated usage of Cassandra from actual programs will behave as expected without having to cover every combination under the sun. Furthermore, extensive unit tests serve as a set of regression tests as well, which helped us to quickly catch any inadvertently introduced bugs during the development of the gateway.

\section*{Attempting to Exploit Information Leaks or Grab Another User's Data}
\addcontentsline{toc}{section}{Attempting to Exploit Information Leaks or Grab Another User's Data}

Additionally, we noticed a handful of queries to system metadata that drivers make as a routine part of connecting to Cassandra that could potentially leak information about other tenants. This information would not allow for a tenant to impact other tenants, but it should be avoided nonetheless. These metadata queries are flagged by the gateway and when results are returned anything not involving the tenant or standard system information is filtered out to prevent leaks.

A tenant exploiting information leaks will be limited to general information like the names of tables or keyspaces, but will be unable to actually query or update data that does not belong to them. This is because any such command will be prefixed with the tenant's own token, so even knowing the name of another tenant's keyspace will not allow anyone else to access it.


\section*{Actual Use of the Cassandra Service}
\addcontentsline{toc}{section}{Actual Use of the Cassandra Service}

We were hoping to provide our Service to other teams to make use of, however this didn't actually happen for a couple of reasons. First, the local Arizona OpenCloud nodes took longer to configure and be ready to use than expected. Second, existing projects that utilize Cassandra, like KairosDB, still use the deprecated ``Thrift'' protocol instead of the newer CQL protocol. Because of this, those programs couldn't use the gateway or our Service.


\chapter*{Outstanding / Unfinished Items}
\addcontentsline{toc}{chapter}{Outstanding / Unfinished Items}

Here we describe outstanding items or ideas that we had talked about implementing but did not have time to complete. We do not believe any of these issues have strong technical reasons that would prevent them from being implemented.

\section*{Missing CQL Specification Items}
\addcontentsline{toc}{section}{Missing CQL Specification Items}

Some optional features of the CQL spec, such as compression of packets and SSL support, are not currently implemented in the gateway. The current drivers do not appear to support or default to these options, so at the moment there isn't any side effect to not having these features implemented.


\section*{Prepared Statements and Information Leakage}
\addcontentsline{toc}{section}{Prepared Statements and Information Leakage}

According to the CQL spec, prepared statements will return a unique uid (uuid) that the client then specifies along with parameters when subsequently executing the command. This uuid is globally unique, and this implies that if one tenant manages to guess another tenant's prepared statement uuid they could execute a prepared statement that does not belong to them. This could allow leaking of information and/or inserting or deleting data that a tenant does not have access to.

This can be mitigated by having each gateway using global state via a table in Cassandra to keep track of which tenant creates a prepared statement and then when attempting to execute a prepared statement enforcing that only the creator tenant can do so. Any other tenant will receive an error and be unable to use a uuid they don't own.


\section*{Keyspace Length Limits}
\addcontentsline{toc}{section}{Keyspace Length Limits}

Internally, Cassandra limits keyspace names to 48 characters. Since we are prefixing each keyspace with a tenant's internal token, this can lead to long keyspace names that normally work suddenly failing when run through the gateway. Short keyspaces will be unaffected. Currently, this limit is 48 - 20 = 28 characters before keyspace creation will fail.

We can work around this limitation either by extending Cassandra's limitation (not sure if this is hardcoded or a limitation of the internal data representation), or by hashing the token prefix plus supplied keyspace like so: sha1sum($<$token$>$ + lowerCase(keyspace)). The second approach would require a secondary table in the multitenantcassandra keyspace to map between hashed keyspace names used internally and the human readable names expected by the end user. This would add a small overhead, but each gateway could cache mappings as needed in RAM to minimize it.


\section*{Accounting}
\addcontentsline{toc}{section}{Accounting}

Because each tenant will have one or more unique tokens and will always be required to authenticate when connecting to the Service, we will be able to track the number of queries executed, size of data transferred, etc for accounting purposes. Furthermore, the tenant will be able to create arbitrary granularity by creating more tokens if needed. We can also track which internal Cassandra user is being used and report it as strictly informative.

Accounting information can be reported on demand individually to each tenant, or completely to the Service administrator / system.

It would not be hard to add this feature to our Service. An accounting table would be created in the multitenantcassandra keyspace, and then each gateway would simply keep track of statistics as packets were processed. To better handle the frequent updates to the observed statistics per connection, the gateway would cache information locally and periodically write updates to Cassandra to ensure the data was properly saved.

\section*{OpenCloud Integration}
\addcontentsline{toc}{section}{OpenCloud Integration}

We have begun work on integrating the Cassandra Service with OpenCloud. Currently we have the SMI completed, and initial work begun on the observer event handling. There's probably just some small piece of magic file or class naming that we are missing before events begin to be triggered as changes are made through the SMI.

\chapter*{Dependencies on Other Services}
\addcontentsline{toc}{chapter}{Dependencies on Other Services}

The Cassandra Service can utilize other existing services to provide a better user experience. These other services include:
\begin{itemize}
\item OpenCloud for tenant authentication and authorization, as well as an API to let our Service dynamically spin up / down nodes as needed to maintain high availability and reliability. By using the observer in OpenCloud, our Service can be dynamic and respond to changes in the environment.
\item Nagios for monitoring the state of individual nodes. When certain thresholds are reached, such as utilization, and event can be generated that will add or remove nodes from the cluster so we can provide good performance.
\item RequestRouter to distribute incoming TCP connections amongst available nodes. RequestRouter can automatically get the IPs of each sliver in the Cassandra slice and allows us to use a nice domain name as the initial contact point for all tenants of our Service.
\end{itemize}


\end{document}
